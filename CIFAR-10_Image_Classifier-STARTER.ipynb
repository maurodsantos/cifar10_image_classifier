{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the CIFAR-10 dataset.\n",
    "\n",
    "To meet the requirements for this project, you will need to achieve an accuracy greater than 45%. \n",
    "If you want to beat Detectocorp's algorithm, you'll need to achieve an accuracy greater than 70%. \n",
    "(Beating Detectocorp's algorithm is not a requirement for passing this project, but you're encouraged to try!)\n",
    "\n",
    "Some of the benchmark results on CIFAR-10 include:\n",
    "\n",
    "78.9% Accuracy | [Deep Belief Networks; Krizhevsky, 2010](https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf)\n",
    "\n",
    "90.6% Accuracy | [Maxout Networks; Goodfellow et al., 2013](https://arxiv.org/pdf/1302.4389.pdf)\n",
    "\n",
    "96.0% Accuracy | [Wide Residual Networks; Zagoruyko et al., 2016](https://arxiv.org/pdf/1605.07146.pdf)\n",
    "\n",
    "99.0% Accuracy | [GPipe; Huang et al., 2018](https://arxiv.org/pdf/1811.06965.pdf)\n",
    "\n",
    "98.5% Accuracy | [Rethinking Recurrent Neural Networks and other Improvements for ImageClassification; Nguyen et al., 2020](https://arxiv.org/pdf/2007.15161.pdf)\n",
    "\n",
    "Research with this dataset is ongoing. Notably, many of these networks are quite large and quite expensive to train. \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list first.\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "CIFAR-10 is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `CIFAR10` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "## YOUR CODE HERE ##\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create training set and define training dataloader\n",
    "## YOUR CODE HERE ##\n",
    "batch_size=8 # tried 64, 8 and 4.\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "## YOUR CODE HERE ##\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# The 10 classes in the dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(classes[labels[i]])\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(np.rot90(image.T, k=3))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "## YOUR CODE HERE ##\n",
    "# inspired from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images (in this case the first batch)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels according to the grid\n",
    "grid_sizes=batch_size\n",
    "if batch_size>8:\n",
    "    grid_sizes=int(np.sqrt(batch_size))\n",
    "pos=range(0, batch_size, grid_sizes)\n",
    "\n",
    "for i in pos:\n",
    "    print(' '.join('%5s' % classes[labels[j]] for j in range(i,i+grid_sizes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset. \n",
    "Feel free to construct a model of any architecture – feedforward, convolutional, or even something more advanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #x = nn.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = Classifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "import torch.optim as optim\n",
    "#The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4) # experimented also with default decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss during each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch: 1/30..  Training Loss: 0.245..  Test Loss: 0.205..  Train Accuracy: 0.272 Test Accuracy: 0.404\n",
      "Epoch: 2/30..  Training Loss: 0.194..  Test Loss: 0.186..  Train Accuracy: 0.438 Test Accuracy: 0.457\n",
      "Epoch: 3/30..  Training Loss: 0.177..  Test Loss: 0.174..  Train Accuracy: 0.494 Test Accuracy: 0.499\n",
      "Epoch: 4/30..  Training Loss: 0.165..  Test Loss: 0.169..  Train Accuracy: 0.531 Test Accuracy: 0.522\n",
      "Epoch: 5/30..  Training Loss: 0.158..  Test Loss: 0.163..  Train Accuracy: 0.554 Test Accuracy: 0.538\n",
      "Epoch: 6/30..  Training Loss: 0.152..  Test Loss: 0.152..  Train Accuracy: 0.574 Test Accuracy: 0.573\n",
      "Epoch: 7/30..  Training Loss: 0.146..  Test Loss: 0.157..  Train Accuracy: 0.589 Test Accuracy: 0.561\n",
      "Epoch: 11/30..  Training Loss: 0.134..  Test Loss: 0.147..  Train Accuracy: 0.627 Test Accuracy: 0.597\n",
      "Epoch: 12/30..  Training Loss: 0.131..  Test Loss: 0.144..  Train Accuracy: 0.634 Test Accuracy: 0.603\n",
      "Epoch: 13/30..  Training Loss: 0.129..  Test Loss: 0.145..  Train Accuracy: 0.637 Test Accuracy: 0.599\n",
      "Epoch: 14/30..  Training Loss: 0.128..  Test Loss: 0.146..  Train Accuracy: 0.640 Test Accuracy: 0.590\n",
      "Epoch: 15/30..  Training Loss: 0.127..  Test Loss: 0.146..  Train Accuracy: 0.643 Test Accuracy: 0.594\n",
      "Epoch: 16/30..  Training Loss: 0.125..  Test Loss: 0.142..  Train Accuracy: 0.651 Test Accuracy: 0.610\n",
      "Epoch: 17/30..  Training Loss: 0.123..  Test Loss: 0.142..  Train Accuracy: 0.655 Test Accuracy: 0.614\n",
      "Epoch: 18/30..  Training Loss: 0.122..  Test Loss: 0.140..  Train Accuracy: 0.657 Test Accuracy: 0.621\n",
      "Epoch: 19/30..  Training Loss: 0.120..  Test Loss: 0.142..  Train Accuracy: 0.663 Test Accuracy: 0.609\n",
      "Epoch: 20/30..  Training Loss: 0.120..  Test Loss: 0.142..  Train Accuracy: 0.663 Test Accuracy: 0.609\n",
      "Epoch: 21/30..  Training Loss: 0.119..  Test Loss: 0.144..  Train Accuracy: 0.666 Test Accuracy: 0.612\n",
      "Epoch: 22/30..  Training Loss: 0.119..  Test Loss: 0.141..  Train Accuracy: 0.665 Test Accuracy: 0.613\n",
      "Epoch: 23/30..  Training Loss: 0.118..  Test Loss: 0.138..  Train Accuracy: 0.667 Test Accuracy: 0.620\n",
      "Epoch: 24/30..  Training Loss: 0.117..  Test Loss: 0.139..  Train Accuracy: 0.670 Test Accuracy: 0.620\n",
      "Epoch: 25/30..  Training Loss: 0.117..  Test Loss: 0.139..  Train Accuracy: 0.673 Test Accuracy: 0.620\n",
      "Epoch: 26/30..  Training Loss: 0.116..  Test Loss: 0.144..  Train Accuracy: 0.676 Test Accuracy: 0.613\n",
      "Epoch: 27/30..  Training Loss: 0.115..  Test Loss: 0.136..  Train Accuracy: 0.676 Test Accuracy: 0.630\n",
      "Epoch: 28/30..  Training Loss: 0.114..  Test Loss: 0.144..  Train Accuracy: 0.680 Test Accuracy: 0.610\n",
      "Epoch: 29/30..  Training Loss: 0.113..  Test Loss: 0.138..  Train Accuracy: 0.680 Test Accuracy: 0.625\n",
      "Epoch: 30/30..  Training Loss: 0.113..  Test Loss: 0.137..  Train Accuracy: 0.682 Test Accuracy: 0.627\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "# move into a cuda-enabled device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "epoch_list = range(epochs)\n",
    "\n",
    "for e in epoch_list:\n",
    "    \n",
    "    tot_train_loss = 0\n",
    "    train_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        #images, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        log_ps = net(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     \n",
    "        tot_train_loss += loss.item()\n",
    "    \n",
    "        # compute train data accuracy\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        train_correct += equals.sum().item()\n",
    "       \n",
    "    else:\n",
    "        tot_test_loss = 0\n",
    "        test_correct = 0  # Number of correct predictions on the test set\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                log_ps = net(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                tot_test_loss += loss.item()\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                test_correct += equals.sum().item()\n",
    "\n",
    "        # Get mean loss to enable comparison between train and test sets\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        test_loss = tot_test_loss / len(testloader.dataset)\n",
    "\n",
    "        # At completion of epoch\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accs.append(train_correct / len(trainloader.dataset))\n",
    "        test_accs.append(test_correct / len(testloader.dataset))\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "              \"Train Accuracy: {:.3f}\".format(train_correct / len(trainloader.dataset)),\n",
    "              \"Test Accuracy: {:.3f}\".format(test_correct / len(testloader.dataset)))\n",
    "\n",
    "PATH = './cifar_small_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_metric_logs(lst_iter, train_loss, train_acc, test_loss, test_acc, title):\n",
    "    plt.plot(lst_iter, train_loss, '-b', label='train loss')\n",
    "    plt.plot(lst_iter, test_loss, '-r', label='val loss')\n",
    "    plt.plot(lst_iter, train_acc, '.b', label='train accuracy')\n",
    "    plt.plot(lst_iter, test_acc, '.r', label='val accuracy')\n",
    "\n",
    "    plt.xlabel(\"n epock\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(title+\".png\")  # should before show method\n",
    "\n",
    "    # show\n",
    "    plt.show()\n",
    "    \n",
    "title = 'metrics_logs_small_network'    \n",
    "    \n",
    "draw_metric_logs(epoch_list, train_losses, train_accs, test_losses, test_accs, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "# various references were studied to achieve this code:\n",
    "# # https://github.com/weiaicunzai/pytorch-cifar100/blob/master/train.py\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "# https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%208%20-%20Transfer%20Learning%20(Solution).ipynb\n",
    "# udacities classes\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "batch_size=16\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "## YOUR CODE HERE ##\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=test_transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Freeze parameters so we don't backprop through them\n",
    "model = torchvision.models.densenet121(pretrained=True)\n",
    "model\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 10))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier =  classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "model.to(device);\n",
    "\n",
    "start = time.time()\n",
    "# move into a cuda-enabled device\n",
    "# device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "# print(device)\n",
    "# net.to(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "epoch_list = range(epochs)\n",
    "EPOCH_MILESTONE = 6;\n",
    "best_acc=0;\n",
    "\n",
    "for e in epoch_list:\n",
    "    \n",
    "    tot_train_loss = 0\n",
    "    train_correct = 0\n",
    "    running_loss = 0.0\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        images, labels = data[0].to(device),data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        log_ps = model.forward(images)\n",
    "        loss = criterion2(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "     \n",
    "        tot_train_loss += loss.item()\n",
    "    \n",
    "        # compute train data accuracy\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        train_correct += equals.sum().item()\n",
    "       \n",
    "    else:\n",
    "        tot_test_loss = 0\n",
    "        test_correct = 0  # Number of correct predictions on the test set\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                log_ps = model.forward(images)\n",
    "                loss = criterion2(log_ps, labels)\n",
    "                tot_test_loss += loss.item()\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                test_correct += equals.sum().item()\n",
    "\n",
    "        # Get mean loss to enable comparison between train and test sets\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        test_loss = tot_test_loss / len(testloader.dataset)\n",
    "\n",
    "        # At completion of epoch\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accs.append(train_correct / len(trainloader.dataset))\n",
    "        acc=test_correct / len(testloader.dataset)\n",
    "        test_accs.append(acc)\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "              \"Train Accuracy: {:.3f}\".format(train_correct / len(trainloader.dataset)),\n",
    "              \"Test Accuracy: {:.3f}\".format(test_correct / len(testloader.dataset)))\n",
    "        \n",
    "        if e > EPOCH_MILESTONE and best_acc < acc:\n",
    "            PATH = \"./cifar10_densenet_\"+str(e)+\"_best\"\n",
    "            print('saving weights file to {}'.format(PATH))\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            best_acc = acc\n",
    "            continue\n",
    "\n",
    "        if not e % EPOCH_MILESTONE:\n",
    "            PATH = \"./cifar10_densenet_\"+str(e)+\"_regular\"\n",
    "            print('saving weights file to {}'.format(PATH))\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "                \n",
    "        finish_epoch = time.time()\n",
    "        print('epoch {} training time consumed: {:.2f}s'.format(e, finish_epoch - start_epoch))\n",
    "        \n",
    "        \n",
    "finish = time.time()\n",
    "\n",
    "print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))\n",
    "\n",
    "PATH = './cifar10_densenet_final.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "title = 'metrics_logs_pre-trained_network'    \n",
    "    \n",
    "draw_metric_logs(epoch_list, train_losses, train_accs, test_losses, test_accs, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 70%, great work! \n",
    "This is a hard task to exceed 70% on.\n",
    "\n",
    "If your accuracy is under 45%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for images, labels in testloader:\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for images, labels in testloader2:\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the pre-trained-densenet network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "# I'm saving the small model here.\n",
    "PATH = './cifar_small_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Recommendation\n",
    "\n",
    "Based on your evaluation, what is your recommendation on whether to build or buy? Explain your reasoning below.\n",
    "\n",
    "Some things to consider as you formulate your recommendation:\n",
    "* How does your model compare to Detectocorp's model?\n",
    "* How does it compare to the far more advanced solutions in the literature? \n",
    "* What did you do to get the accuracy you achieved? \n",
    "* Is it necessary to improve this accuracy? If so, what sort of work would be involved in improving it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Double click this cell to modify it**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Your Project\n",
    "\n",
    "When you are finished editing the notebook and are ready to turn it in, simply click the **SUBMIT PROJECT** button in the lower right.\n",
    "\n",
    "Once you submit your project, we'll review your work and give you feedback if there's anything that you need to work on. If you'd like to see the exact points that your reviewer will check for when looking at your work, you can have a look over the project [rubric](https://review.udacity.com/#!/rubrics/3077/view)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
